\subsection{Complexity}

In the last section, we introduced the object of the analysis.
In this section, we define the measures which should be computed with the analysis.
Therefore, we introduce the terms of time, size and cost complexity.
Time complexity describes how many steps an evaluation of a program will take in a worst-case scenario.
The costs of a transition are not considered in this complexity type.
On the other hand, cost complexity considers the costs of transitions and describes the cost of the sequence of transitions taken in a worst-case run.
Time complexity can be seen as a special case of cost complexity, where the cost $c(t)$ of every transition $t \in \TSet$ equals $1$.
The third complexity type is size complexity.
For each transition the size complexity yields an assignment from each variable to an interval, which defines the range of the value of the variable after the execution of the transition in an arbitrary run.

\subsubsection{Time Complexity}

We define the time complexity of a program $\Program$ as the highest possible number of steps in an arbitrary run with an initial state $\valuation_0 \in \Valuation$ between a lower and an upper bound $\lstate \leq \valuation_0 \leq \ustate$.

\begin{definition}[Worst-Case Time Complexity]
  We call $\text{rc}: (\Valuation \times \Valuation) \rightarrow \mathbb{N}$ the \textbf{time complexity} of a program if and only if for all states $\lstate, \ustate \in \Valuation$ it holds that
  \[ \text{rc}(\lstate, \ustate) = \timecomplexityterm \]
\end{definition}

An upper time bound of a transition $t \in \TSet$ describes a maximal number of occurrences of that transition in an evaluation starting with an arbitrary initial state $\valuation_0 \in \Valuation$ between a lower and an upper bound $\lstate \leq \valuation_0 \leq \ustate$.

\begin{definition}[Upper Time Bound]
  We call $\UTime: \TSet \rightarrow \BoundSet(\PVSet)$ an \textbf{upper time bound} if and only if for all $t \in \TSet$ and all states $\lstate, \ustate \in \Valuation$ it holds that
  \[ \ueval{\UTime(t)}{\lstate}{\ustate} \geq \timeboundterm \]
  We also refer to an upper time bound as \textbf{time bound}, since lower time bounds are not considered in this thesis.
\end{definition}

The original KoAT defines time complexity as $\text{rc}'(m) = \sup \braced{ k \in \mathbb{N} \mid \exists \valuation_0, \location, \valuation: \abs{\valuation_0} \leq m \wedge (\location_0, \valuation_0) \rightarrow^k (\location, \valuation) }$.
This is a special case of the new definition.

\begin{remark}[Original KoAT time complexity]
  Let $-\valuation$ denote the negation $(-\valuation)(v) = -\valuation(v)$ of the value of every variable $v \in \PVSet$ for a state $\valuation$.
  Let $\text{rc}'(m) = \sup \braced{ k \in \mathbb{N} \mid \exists \valuation_0, \location, \valuation: \abs{\valuation_0} \leq m \wedge (\location_0, \valuation_0) \rightarrow^k (\location, \valuation) }$ be the time complexity definition of the original KoAT.
  Then, for every state $m \in \Valuation$ it holds that
  \[ \text{rc}'(m) = \text{rc}(-m,m). \]
\end{remark}

This also shows the potentials of the new time complexity definition.
With different choices for the states $\lstate, \ustate \in \Valuation$ such that $-\lstate \neq \ustate$ the time complexity of a program might be lower than with a single state $m \in \Valuation$.

Consider the motivational program in figure \ref{fig:motivational_example}.
Let $\lstate, \ustate \in \Valuation$ be states with $\ustate(x) = 4$, $\ustate(y) = 2$, $\lstate(x) = -6$ and $\lstate(y) = 0$.
An input state for the original KoAT would therefore be an $m \in \Valuation$ with $m(x) = 6$ and $m(y) = 2$.
The time complexity of the original KoAT would then be $\text{rc}'(m) = 9$, since with a state $\valuation'_0 \in \Valuation$ with $\valuation'_0(x) = 6$ and $\valuation'_0(y) = -2$ (and therefore $\abs{\valuation'_0} \leq m$) we have one occurrence of $t_0$ and eight occurrences of $t_1$.
Since $\lstate \nleq \valuation'_0 \nleq \ustate$, the state $\valuation'_0$ is not a possible input state for the time complexity $\text{rc}$.
Instead, the input state yielding the greatest numbers of evaluation steps is $\valuation_0 \in \Valuation$ with $\valuation_0(x) = 4$ and $\valuation_0(y) = 0$.
Therefore, the new time complexity $\text{rc}(\lstate, \ustate)$ is $1 + 4$, which is significantly lower than the time complexity of the original KoAT $\text{rc}'(m) = 9$.

Although we changed the definitions of time complexity and time bounds, the known theorem, that it is possible to approximate the time complexity of a program by the sum of all upper time bounds, is not affected.

\input{theorems/approximating_rc}

Therefore, it is sufficient to determine an upper time bound for a program and build the sum over all transitions $\TSet$ of the program, to approximate the time complexity. 

\subsubsection{Size complexity}

A size bound of a program defines for each variable at a particular transition an interval in which the value ranges in a worst-case evaluation.
An interval is defined by a lower size bound and an upper size bound.
While upper size bounds are always higher than the highest possible value at a transition, lower size bounds are always smaller than the lowest possible value.

\begin{definition}[Worst-Case Size Bound]
  Let $\RV = \TSet \cup \VSet$ be the set of all result variables.
  We call $\USize: \RV \rightarrow \BoundSet(\PVSet)$ an \textbf{upper size bound} if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \ueval{\USize(t, v)}{\lstate}{\ustate} \geq \usizeboundterm. \]
  Furthermore, we call $\LLSB: \RV \rightarrow \BoundSet(\PVSet)$ a \textbf{lower size bound} if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \leval{\LSize(t, v)}{\lstate}{\ustate} \leq \lsizeboundterm. \]
  Then, we call $\Size$ a size bound.
\end{definition}

Note that for a transition $t = (\location,\text{id},\guard,\location') \in \TSet$, the upper size bound $\USize(t,x) = x$ is identical to the lower size bound $\LSize(t,x) = x$.
Different upper and lower size bounds result from further restrictions on the incoming variables.
With $\guard = \braced{x \geq 0}$, we can determine $\USize(t,x) = x$ as an upper bound and $\USize(t,x) = 0$ as a lower bound.

The presented definition of size bounds expresses a bound depending on the values at the start of the program.
For the definition of the methods for the computation of trivial and nontrivial size bounds, we also need a definition of bounds depending on the values immediately before the execution of a transition.
We then use those local size bounds to lift them in a global context.

\begin{definition}[Local Size Bound]
  We call $\ULSB: \RV \rightarrow \BoundSet(\PVSet)$ an \textbf{upper local size bound} if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \ueval{\ULSB(t, v)}{\lstate}{\ustate} \geq \ulocalsizeboundterm. \]
  Furthermore, we call $\LLSB: \RV \rightarrow \BoundSet(\PVSet)$ a \textbf{lower local size bound} if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \leval{\LLSB(t, v)}{\lstate}{\ustate} \leq \llocalsizeboundterm. \]
  We then call $\LSB$ a local size bound.
\end{definition}

\input{graphs/localglobal}

Consider figure \ref{fig:localglobal}.
While $\ULSB(t_1, x) = 2 \cdot x$ is a valid upper local size bound for the result variable $(t_1, x)$, it only describes the value of $x$ in terms of the value immediately before the execution of the transition $t_1$.
An upper global size bound instead expresses the value of $x$ in terms of the initial values of the program.
Therefore $\USize(t_1, x) = 2 \cdot (x + 1)$ is a valid upper global size bound for the result variable $(t_1, x)$.

Throughout this master's thesis, we use the application of a global or local size bound as a function for different purposes.
Let $f: \RV \rightarrow \BoundSet(\PVSet)$ be an arbitrary size bound.
Then, we denote with $f(\alpha)$ for a result variable $\alpha \in \RV$ the trivial application of the function $f$ to the argument $\alpha$ which results in a bound $f(\alpha) \in \BoundSet(\PVSet)$.
We use the abbreviation $f(t, v)$ for a transition $t \in \TSet$ and a variable $v \in \VSet$ to denote the application $f((t,v)) \in \BoundSet(\PVSet)$ with $(t,v) \in \RV$.
If the function $f$ is only applied to a transition $t \in \TSet$, this denotes the partial application of $f$ to $t$ resulting in $f(t): \VSet \rightarrow \BoundSet(\PVSet)$.

\subsubsection{Cost Complexity}

Additionally to time and size complexity, we consider cost complexity.
The cost complexity of a program is defined as the highest sum of the costs of a transition sequence of an arbitrary evaluation starting in an input state $\valuation_0 \in \Valuation$.

\begin{definition}[Worst-Case Cost Complexity]
\[ \text{cc}(\lstate, \ustate) = \costcomplexityterm \]
\end{definition}

The definition of cost complexity is useful for a modular analysis, where a transition is able to describe the whole effect of a subprogram.
Then, a transition might occur only a specific number of times in the outer program, but in fact every occurrence is not just a link to a single transition usage, but to a complete execution of a subprogram with an own time bound.
Lifting those time bounds into the context of the outer program, we consider them as cost of the transition of the outer program. \todo{Example or later?}{}
For that reason, the cost function $\cost: \TSet \rightarrow \BoundSet(\PVSet)$ is defined for the whole bound set $\BoundSet(\PVSet)$.

Unfortunately, the usefulness of the distinction between upper and lower size bounds is limited for the computation of a cost bound.
Note that for a transition $t \in \TSet$ the costs $\cost(t)$ are defined as local costs.
For the computation of a cost bound we need to lift them similar as the local size bounds to a global context.
Therefore, it is necessary to substitute the variables of the costs $\cost(t)$ with the appropriate size bound computed so far.
Since $\cost(t) \in \BoundSet(\PVSet)$ is in general not a monotonic function, this substitution is not trivial.
In fact, we are only able to use the distinction between upper and lower size bounds for affine costs $\cost(t) \in \BoundSet_a(\PVSet)$.
For arbitrary costs $\cost(t) \in \BoundSet(\PVSet) \setminus \BoundSet_a(\PVSet)$ it is necessary to consider the monotonic approach from the original KoAT.

\begin{definition}[Upper Cost Bound]
  We call $\UCost: \TSet \rightarrow \BoundSet(\PVSet)$ an \textbf{upper cost bound} if and only if for all $t \in \TSet$ and all states $\lstate, \ustate \in \Valuation$ it holds that
  \[ \ueval{\UCost(t)}{\lstate}{\ustate} \geq \costboundterm \]
  We also refer to an upper cost bound as \textbf{cost bound}, since lower cost bounds are not considered in this thesis.
\end{definition}

Similar to time bounds, it is also possible to approximate the cost complexity of a program with the sum of cost bounds for all transitions in $\TSet$.

\input{theorems/approximating_cc}
