\subsection{Complexity}

In the last section, we introduced the object of the analysis.
In this section, we define the measures we want to compute with the analysis.
Therefore, we introduce the terms of time, size and cost complexity.
Time complexity describes how many steps an evaluation of a program will take in a worst-case scenario.
The costs of a transition are not considered in this complexity type.
On the other hand, cost complexity considers the costs of transitions and describes the cost of the sequence of transitions taken in a worst-case run.
The third complexity type is size complexity.
Size complexity yields for each transition an assignment from each variable to an interval, in which its value ranges after the execution of the transition in an arbitrary run.

\subsubsection{Time Complexity}

We define the time complexity of a program $\Program$ as the highest possible number of steps in an arbitrary run with an arbitrary initial state $\valuation_0 \in \Valuation$.

\begin{definition}[Worst-Case Time Complexity]
  We call $\text{rc} \in \BoundSet$ the time complexity of a program if and only if for all initial states $\valuation_0 \in \Valuation$ it holds that
  \[ \eval{\text{rc}}{\valuation_0} = \sup \braced{ k \in \mathbb{N} \mid \exists \location, \valuation: (\location_0, \valuation_0) \rightarrow^k (\location, \valuation) } \]
\end{definition}

An upper time bound of a transition $t \in \TSet$ describes a maximal number of occurrences of that transition in an evaluation starting with an arbitrary initial state $\valuation_0 \in \Valuation$.

\begin{definition}[Upper Time Bound]
  We call $\UTime: \TSet \rightarrow \BoundSet$ a time bound if and only if for all $t \in \TSet$ and all initial states $\valuation_0 \in \Valuation$ it holds that
  \[ \eval{\UTime(t)}{\valuation_0} \geq \sup \braced{ k \in \mathbb{N} \mid \exists \location, \valuation: (\location_0, \valuation_0) (\rightarrow^* \circ \rightarrow_t)^k (\location, \valuation) } \]
\end{definition}

Those definitions are slightly different than in related work. \todo{Source needed?}{}
Often time complexity is considered to be a function which is monotonic in its input state $\eval{\text{rc}'}{m} = \sup \braced{ k \in \mathbb{N} \mid \exists \valuation_0, \location, \valuation: \abs{\valuation_0} \leq m \wedge (\location_0, \valuation_0) \rightarrow^k (\location, \valuation) }$.
This guarantees that for an input $m \in \Valuation$ there is no other input $m' \leq m$ with $\eval{\text{rc}'}{m'} > \eval{\text{rc}'}{m}$.
Unfortunately, this property can not be guaranteed anymore with non-monotonic bounds.
Consider the motivational program in figure \ref{fig:motivational_example} and an input state $m \in \Valuation$ with $m(x) = 4$ and $m(y) = 2$.
The time complexity according to the definition in related work would then be $\eval{\text{rc}'}{m} = 7$, since with a state $\valuation_0 \in \Valuation$ with $\valuation_0(x) = 4$ and $\valuation_0(y) = -2$ we have one occurrence of $t_0$ and six occurrences of $t_1$.
As mentioned before, the new method should be able to yield a time bound $1 + \maxO{x-y}$.
But for the defined input state $m \in \Valuation$ this would yield $1 + \maxO{4-2} = 3$, which would be unsound according to the definition of time complexity.
With the changed definition the time complexity is $\eval{\text{rc}}{m} = 1 + 2$.
Therefore the non-monotonic bound is sound according to this definition.

Although we changed the definitions of time complexity and time bounds, the known theorem, that it is possible to approximate the time complexity of a program by the sum of all upper time bounds, is not affected.

\input{theorems/approximating_rc}

Therefore, it is sufficient to determine an upper time bound for a program and build the sum over all transitions $\TSet$ of the program, to approximate the time complexity. 

\subsubsection{Size complexity}

A size bound of a program defines for each variable at a particular transition an interval in which the value ranges in a worst-case evaluation.
An interval is defined by a lower size bound and an upper size bound.
While upper size bounds are always higher than the highest possible value at a transition, lower size bounds are always smaller than the lowest possible value.

\begin{definition}[Worst-Case Size Bound]
  Let $\RV = \TSet \cup \VSet$ be the set of all result variables.
  We call $\USize: \RV \rightarrow \BoundSet$ an \textbf{upper} size bound if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \eval{\USize(t, v)}{\valuation_0} \geq \sup \braced{\valuation(v) \mid \exists \location, \valuation: (\location_0, \valuation_0) (\rightarrow^* \circ \rightarrow_t) (\location, \valuation)}. \]
  Furthermore, we call $\LLSB: \RV \rightarrow \BoundSet$ a \textbf{lower} size bound if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \eval{\LSize(t, v)}{\valuation_0} \geq \inf \braced{\valuation(v) \mid \exists \location, \valuation: (\location_0, \valuation_0) (\rightarrow^* \circ \rightarrow_t) (\location, \valuation)}. \]
  Then, we call $\Size$ a size bound.
\end{definition}

Note that for a transition $t = (\location,\text{id},\guard,\location') \in \TSet$, the upper size bound $\USize(t,x) = x$ is identical to the lower size bound $\LSize(t,x) = x$.
Different upper and lower size bounds result from further restrictions on the incoming variables.
With $\guard = \braced{x \geq 0}$, we can determine $\USize(t,x) = x$ as an upper bound and $\USize(t,x) = 0$ as a lower bound.

The presented definition of size bounds expresses a bound depending on the values at the start of the program.
For the definition of the methods for the computation of trivial and nontrivial size bounds, we also need a definition of bounds depending on the values immediately before the execution of a transition.
We then use those local size bounds to lift them in a global context.

\begin{definition}[Local Size Bound]
  We call $\ULSB: \RV \rightarrow \BoundSet$ an \textbf{upper} local size bound if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \eval{\ULSB(t, v)}{\valuation} \geq \sup \braced{\valuation'(v) \mid \exists \location, \location', \valuation': (\location, \valuation) \rightarrow_t (\location', \valuation')}. \]
  Furthermore, we call $\LLSB: \RV \rightarrow \BoundSet$ a \textbf{lower} local size bound if and only if for every result variable $(t, v) \in \RV$ and every state $\valuation \in \Valuation$ it holds that
  \[ \eval{\LLSB(t, v)}{\valuation} \geq \inf \braced{\valuation'(v) \mid \exists \location, \location', \valuation': (\location, \valuation) \rightarrow_t (\location', \valuation')}. \]
  We then call $\LSB$ a local size bound.
\end{definition}

\input{graphs/localglobal}

Consider figure \ref{fig:localglobal}.
While $\ULSB(t_1, x) = 2 \cdot x$ is a valid upper local size bound for the result variable $(t_1, x)$, it only describes the value of $x$ in terms of the value immediately before the execution of the transition $t_1$.
An upper global size bound instead expresses the value of $x$ in terms of the initial values of the program.
Therefore $\USize(t_1, x) = 2 \cdot (x + 1)$ is a valid upper global size bound for the result variable $(t_1, x)$.

Throughout this master's thesis, we use the application of a global or local size bound as a function for different purposes.
Let $f: \RV \rightarrow \BoundSet$ be an arbitrary size bound.
Then, we denote with $f(\alpha)$ for a result variable $\alpha \in \RV$ the trivial application of the function $f$ to the argument $\alpha$ which results in a bound $f(\alpha) \in \BoundSet$.
We use the abbreviation $f(t, v)$ for a transition $t \in \TSet$ and a variable $v \in \VSet$ to denote the application $f((t,v)) \in \BoundSet$ with $(t,v) \in \RV$.
If the function $f$ is only applied to a transition $t \in \TSet$, this denotes the partial application of $f$ to $t$ resulting in $f(t): \VSet \rightarrow \BoundSet$.

\subsubsection{Cost Complexity}

Additionally to time and size complexity, we consider cost complexity.
The cost complexity of a program is defined as the highest sum of the costs of a transition sequence of an arbitrary evaluation starting in an input state $\valuation_0 \in \Valuation$.

\begin{definition}[Worst-Case Cost Complexity]
\[ \eval{\text{cc}}{\valuation_0} = \sup \braced{ \sum_{0 \leq i \leq k} \eval{\cost(t_i)}{\valuation_i} \mid \exists k \geq 1: 
  (\location_0, \valuation_0) \rightarrow_{t_0} (\location_1, \valuation_1) \rightarrow_{t_1} \dots \rightarrow_{t_k} (\location_k, \valuation_k) } \]
\end{definition}

The definition of cost complexity is useful for a modular analysis, where a transition is able to describe the whole effect of a subprogram.
Then, a transition might occur only a specific number of times in the outer program, but in fact every occurrence is not just a link to a single transition usage, but to a complete execution of a subprogram with an own time bound.
Lifting those time bounds into the context of the outer program, we consider them as cost of the transition of the outer program. \todo{Example or later?}{}
For that reason, the cost function $\cost: \TSet \rightarrow \BoundSet$ is defined for the whole bound set $\BoundSet$.

Unfortunately, the usefulness of the distinction between upper and lower size bounds is limited for the computation of a cost bound.
Note that for a transition $t \in \TSet$ the costs $\cost(t)$ are defined as local costs.
For the computation of a cost bound we need to lift them similar as the local size bounds to a global context.
Therefore, it is necessary to substitute the variables of the costs $\cost(t)$ with the appropriate size bound computed so far.
Since $\cost(t) \in \BoundSet$ is in general not a monotonic function, this substitution is not trivial.
In fact, we are only able to use the distinction between upper and lower size bounds for affine costs $\cost(t) \in \BoundSet_a$.
For arbitrary costs $\cost(t) \in \BoundSet \setminus \BoundSet_a$ it is necessary to consider the monotonic approach from the original KoAT.

\begin{definition}[Upper Cost Bound]
  Let $\abs{\mathcal{S}}(t, v)$ denote the highest absolute value $\max(\USize(t, v), -\LSize(t, v))$ a variable $v$ can reach at a transition $t$.
  We call $\UCost: \TSet \rightarrow \BoundSet$ a cost bound if and only if for all $t \in \TSet$ it holds that
  \[ \UCost(t) = \UTime(t) \cdot
  \begin{cases}
    \maxO{\cost(t)} & \text{if } t \in \TSet_0 \\
    \maxO{\maximum{\subst{\cost(t)}{\LSize(\pret)}{\USize(\pret)} \mid \pret \in \pre(t)}} & \text{if } \cost(t) \in \BoundSet_a \\
    \maxO{\maximum{\subst{\abs{\cost(t)}}{\abs{\mathcal{S}}(\pret)}{} \mid \pret \in \pre(t)}} & \text{otherwise} \\
  \end{cases}
  \]
\end{definition}

Similar to time bounds, it is also possible to approximate the cost complexity of a program with the sum of cost bounds for all transitions in $\TSet$.

\input{theorems/approximating_cc}
